{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNTbdc10faaCGYNrmAc2hSY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Visual pipeline : CLIP_Adapter"
      ],
      "metadata": {
        "id": "3P54GBX818ha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install CLIP"
      ],
      "metadata": {
        "id": "AMB2dLFw4Wga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "QeFeU8E04Y5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Loading function for dataset loading"
      ],
      "metadata": {
        "id": "UzMjJz9ega28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pickle\n",
        "os.chdir('/content/drive/MyDrive/Dataset') #need to change this directory to your specific directory\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import requests\n",
        "import clip\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "global classes\n",
        "\n",
        "classes = [\"Drama\", \"Comedy\", \"Romance\", \"Thriller\", \"Crime\", \"Action\", \"Adventure\", \"Horror\", \"Documentary\", \"Mystery\", \"Sci-Fi\", \"Music\", \"Fantasy\", \"Family\", \"Biography\", \"War\", \"History\", \"Animation\", \"Musical\", \"Western\", \"Sport\", \"Short\", \"Film-Noir\"]\n",
        "\n",
        "def load_small_set():\n",
        "\n",
        "  # load the dataset from .pkl file\n",
        "  with open('mid_set.pkl', 'rb') as file:\n",
        "      small_set = pickle.load(file)\n",
        "  print('test small_set!!!')\n",
        "  print(small_set[0][0])\n",
        "  print(type(small_set[0]))\n",
        "  print('pass!!!')\n",
        "  batch_size = 32\n",
        "  # print(small_set)\n",
        "\n",
        "  processed_small_set = []\n",
        "  test_debug = []\n",
        "\n",
        "  for img, label, index in small_set:\n",
        "      processed_img = preprocess(img).to(device)\n",
        "      processed_data = (processed_img, label, index)\n",
        "      processed_small_set.append(processed_data)\n",
        "      test_debug.append(index)\n",
        "      # print('test!!! size of preprocessed image!')\n",
        "      # print(processed_img.shape)\n",
        "  small_set = processed_small_set\n",
        "  # creae train_loader and test_loader\n",
        "  train_loader = DataLoader(small_set[0:5000], batch_size=batch_size, shuffle=True)\n",
        "  print(test_debug)\n",
        "  test_loader = DataLoader(small_set[5000:6000], batch_size=batch_size, shuffle=False)\n",
        "  return train_loader, test_loader\n",
        "\n",
        "# train_loader, test_loader = load_small_set() #uncomment this if want to examine in 2.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI-nCM4ugifG",
        "outputId": "8056f8ba-cc9e-439d-d362-4ee550bd76e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 (alternative) Examine the loaded data(if want to run, need to uncomment the final line in section2)"
      ],
      "metadata": {
        "id": "oHjy_ssAdNF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this examination program will show some samples in the set"
      ],
      "metadata": {
        "id": "p8d-kqryUegx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_images_to_display = 100\n",
        "rows, cols = 10, 10\n",
        "displayed_images = 0\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(10, 10))\n",
        "\n",
        "for batch_idx, (images, labels, idex) in enumerate(train_loader):\n",
        "    batch_size = images.shape[0]\n",
        "    for i in range(batch_size):\n",
        "        if displayed_images >= num_images_to_display:\n",
        "            break\n",
        "        image = images[i].permute(1, 2, 0)\n",
        "        ax = axs[displayed_images // cols, displayed_images % cols]\n",
        "        ax.imshow(image)\n",
        "        ax.axis('off')\n",
        "        displayed_images += 1\n",
        "\n",
        "    if displayed_images >= num_images_to_display:\n",
        "        break\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9ntgF3l5dPWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Main body of the training code"
      ],
      "metadata": {
        "id": "nMViFH_hhNja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import clip\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os\n",
        "import datetime\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "# preprocess the prompt\n",
        "text_inputs = torch.cat([clip.tokenize(f\"a poster of a {c} movie\") for c in classes]).to(device)\n",
        "\n",
        "# this is the model class for the whole visual pipeline\n",
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiModalModel, self).__init__()\n",
        "        self.adapter = Adapter(512, 4).to(clip_model.dtype)\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "        self.dtype = clip_model.dtype\n",
        "    def forward(self, img, text_inputs):\n",
        "\n",
        "        # Calculate CLIP_features\n",
        "        with torch.no_grad():\n",
        "            img = img.to(device)\n",
        "            text_inputs = text_inputs.to(device)\n",
        "            image_features = clip_model.encode_image(img)\n",
        "            text_features = clip_model.encode_text(text_inputs)\n",
        "\n",
        "        x = self.adapter(image_features)\n",
        "        y = self.adapter(text_features)\n",
        "        ratio = 0.2\n",
        "        image_features = ratio * x + (1 - ratio) * image_features\n",
        "        text_features = ratio * y + (1 - ratio) * text_features\n",
        "\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "        # print(image_features)\n",
        "        # print(text_features)\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits = (100 * image_features @ text_features.t()).softmax(dim=-1)\n",
        "        # logits = torch.softmax(logits)\n",
        "        # print(logits)\n",
        "        # logits = (100 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# this is just the class for Adapter module in the pipeline\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, c_in, reduction=4):\n",
        "        super(Adapter, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(c_in, c_in // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(c_in // reduction, c_in, bias=False),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.fc = self.fc.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# #######################Training function####################################\n",
        "def train_model(model, train_loader, optimizer, criterion, num_epochs, writer):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for img, labels, idex in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(img, text_inputs)\n",
        "            labels = labels.to(device)\n",
        "            loss = criterion(outputs, labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "        writer.add_scalar('Training Loss', avg_epoch_loss, epoch+1)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}\")\n",
        "    writer.close()\n",
        "\n",
        "def main():\n",
        "    # setups\n",
        "    image_feature_dim = 512\n",
        "    text_feature_dim = 512\n",
        "    num_classes = 23\n",
        "    # hidden_dim = ...\n",
        "    num_epochs = 30\n",
        "    learning_rate = 0.1\n",
        "\n",
        "    # loading the dataset costs lots of RAM\n",
        "    train_loader, test_loader = load_small_set()\n",
        "\n",
        "    model = MultiModalModel()\n",
        "\n",
        "    model_checkpoint_path = 'CLIP_Adapter_3layers_15epoch_best.pth'  # loading previously-trained weight\n",
        "    if os.path.exists(model_checkpoint_path):\n",
        "        model.load_state_dict(torch.load(model_checkpoint_path))\n",
        "        print(f\"Model parameters loaded from '{model_checkpoint_path}'\")\n",
        "\n",
        "    print('Turning off gradients in both the image and the text encoder')\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'adapter' not in name:\n",
        "            param.requires_grad_(False)\n",
        "######uncomment the whole section below if wants to train the model#############\n",
        "    # # define cost function and optimization\n",
        "    # criterion = nn.BCELoss()\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    # #tensorboard launching\n",
        "    # writer = SummaryWriter()\n",
        "    # # Training starts\n",
        "    # train_model(model, train_loader, optimizer, criterion, num_epochs, writer)\n",
        "    # # get the current date and time\n",
        "    # current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "    # #naming checkpoint file\n",
        "    # model_checkpoint_path = f'/content/model_checkpoint_{current_time}.pth'\n",
        "\n",
        "    # # saving training results\n",
        "    # torch.save(model.state_dict(), model_checkpoint_path)\n",
        "    # print(f\"Model parameters saved to '{model_checkpoint_path}'\")\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "Vofn5Koa1_kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. load checkpoint to test F-score.\n",
        "(the best performance is saved in 'CLIP_Adapter_3layers_15epoch_best.pth')"
      ],
      "metadata": {
        "id": "UatW9omkZ9nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model_checkpoint_path = 'CLIP_Adapter_3layers_15epoch_best.pth'\n",
        "model = MultiModalModel()\n",
        "model.load_state_dict(torch.load(model_checkpoint_path))\n",
        "model.to(device)\n",
        "\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for img, labels, idex in test_loader:\n",
        "        outputs = model(img, text_inputs)\n",
        "        test_labels.append(labels)\n",
        "        test_debug =[]\n",
        "        for i in range(len(img)):\n",
        "          top_values = []\n",
        "          top_indices = []\n",
        "          top_values, top_indices = outputs[i].topk(3)\n",
        "          selected_indices = []\n",
        "          # Apply conditions to select final labels\n",
        "          # Apply conditions to select final labels\n",
        "          #post-process rule A as mentioned in the report section 3.1\n",
        "          if top_values[0] >= 50.0:\n",
        "              selected_indices = [top_indices[0]]\n",
        "          elif top_values[0] + top_values[1] >= 50.0:\n",
        "              selected_indices = top_indices[:2]\n",
        "          else:\n",
        "              selected_indices = top_indices\n",
        "          # Print the result\n",
        "          # print(\"\\nTop predictions:\\n\")\n",
        "          predicted_after_onehot = torch.zeros(23)\n",
        "\n",
        "          selected_indices = selected_indices\n",
        "          for index in selected_indices:\n",
        "            predicted_after_onehot[index] = 1\n",
        "          test_predictions.append(predicted_after_onehot)\n",
        "\n",
        "test_labels = torch.cat(test_labels, dim=0)\n",
        "test_predictions = torch.stack(test_predictions, dim=0)\n",
        "\n",
        "f1 = f1_score(test_labels, test_predictions, average=\"weighted\")\n",
        "print('weighted F1-score:', f1)\n",
        "microf1 = f1_score(test_labels, test_predictions, average=\"micro\")\n",
        "print('micro F1-score:', microf1)\n",
        "macrof1 = f1_score(test_labels, test_predictions, average=\"macro\")\n",
        "print('macro F1-score:', macrof1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZnp7gGjaGor",
        "outputId": "017f4aec-95ae-4ed0-d75a-5db085a11bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weighted F1-score: 0.499638253846981\n",
            "micro F1-score: 0.5093755689058802\n",
            "macro F1-score: 0.38535284705176304\n"
          ]
        }
      ]
    }
  ]
}