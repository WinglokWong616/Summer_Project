{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMoJCOi54kjvMhPZJHi/zlF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. install transformers"
      ],
      "metadata": {
        "id": "xhcVbYN11rrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "_nrdFyMX8x-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. main body of the training code"
      ],
      "metadata": {
        "id": "C3TGMHglB-H4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import datetime\n",
        "import transformers as tformer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "os.chdir('/content/drive/MyDrive/Dataset')\n",
        "\n",
        "def train_model(model, train_loader, optimizer, criterion, num_epochs, writer):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for (plots, genres_labels) in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            plots = pd.Series(plots)\n",
        "            tokenized = plots.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "            #truncation to fit the 512 length requirement\n",
        "            max_sequence_length = 768\n",
        "            tokenized = tokenized.apply(lambda seq: seq[:max_sequence_length])\n",
        "            max_len = 0\n",
        "            for i in tokenized.values:\n",
        "                if len(i) > max_len:\n",
        "                    max_len = len(i)\n",
        "            padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "            attention_mask = np.where(padded != 0, 1, 0)\n",
        "            input_ids = torch.tensor(padded)\n",
        "            attention_mask = torch.tensor(attention_mask)\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            genres_labels = genres_labels.to(device)\n",
        "            loss = criterion(outputs, genres_labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "        writer.add_scalar('Training Loss', avg_epoch_loss, epoch+1)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}\")\n",
        "    writer.close()\n",
        "\n",
        "def load_8k_set():\n",
        "  pickle_filename = '8k_dataset.pkl'\n",
        "\n",
        "  with open(pickle_filename, 'rb') as file:\n",
        "      small_set = pickle.load(file)\n",
        "\n",
        "  # print('pass!!!')\n",
        "  batch_size = 64\n",
        "\n",
        "  processed_data8k = []\n",
        "  test_debug = []\n",
        "\n",
        "  for movie in small_set:\n",
        "\n",
        "      genre_label = torch.tensor(movie['genre'])\n",
        "      plot_genre = (movie['plot'], genre_label)\n",
        "      processed_data8k.append(plot_genre)\n",
        "  # create train_loader\n",
        "  train_loader = DataLoader(processed_data8k[0:7000], batch_size=batch_size, shuffle=True)\n",
        "  # loading test data\n",
        "  test_loader = DataLoader(processed_data8k[7000:8000], batch_size=batch_size, shuffle=False)\n",
        "  return train_loader, test_loader\n",
        "#this modele class below stands for the whole text pipeline\n",
        "class DistilBert(nn.Module):\n",
        "    def __init__(self, c_in):\n",
        "        super(DistilBert, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(c_in, 512, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 320, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(320, 120, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(120, 23, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.BERTog = model_class.from_pretrained(pretrained_weights)\n",
        "        # self.BERTog = AutoModelForSequenceClassification.from_pretrained(\"zayedupal/movie-genre-prediction_distilbert-base-uncased\")\n",
        "        self.fc = self.fc.to(device)\n",
        "        self.BERTog = self.BERTog.to(device)\n",
        "        # freeze parameters\n",
        "        for param in self.BERTog.parameters():\n",
        "            param.requires_grad = False\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        last_hidden_states = self.BERTog(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        features = last_hidden_states[0][:,0,:]\n",
        "        # print(\"!!!!features是\", features)\n",
        "        x = self.fc(features)\n",
        "        return x\n",
        "\n",
        "num_classes = 23\n",
        "# hidden_dim = ...\n",
        "num_epochs = 10\n",
        "learning_rate = 0.00005\n",
        "# to create a loader object\n",
        "train_loader, test_loader = load_8k_set()\n",
        "\n",
        "# For DistilBERT:\n",
        "model_class, tokenizer_class, pretrained_weights = (tformer.DistilBertModel, tformer.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model_og = model_class.from_pretrained(pretrained_weights)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"zayedupal/movie-genre-prediction_distilbert-base-uncased\")\n",
        "# model_og = AutoModelForSequenceClassification.from_pretrained(\"zayedupal/movie-genre-prediction_distilbert-base-uncased\")\n",
        "\n",
        "# create a model object\n",
        "model = DistilBert(768)\n",
        "#################main training code#############################\n",
        "# cost function and optimization choice\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#tensorboard launching\n",
        "writer = SummaryWriter()\n",
        "model_checkpoint_path = 'model_checkpoint_2023-08-27_21-18-33.pth'  # load previous results\n",
        "if os.path.exists(model_checkpoint_path):\n",
        "    model.load_state_dict(torch.load(model_checkpoint_path))\n",
        "    print(f\"Model parameters loaded from '{model_checkpoint_path}'\")\n",
        "\n",
        "train_model(model, train_loader, optimizer, criterion, num_epochs, writer)\n",
        "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "model_checkpoint_path = f'/content/model_checkpoint_{current_time}.pth'\n",
        "\n",
        "# name the checkpoints with current date and time\n",
        "torch.save(model.state_dict(), model_checkpoint_path)\n",
        "print(f\"Model parameters saved to '{model_checkpoint_path}'\")\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs\n",
        "###############################\n"
      ],
      "metadata": {
        "id": "A99h8CkFCAqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Testing - load essential function"
      ],
      "metadata": {
        "id": "g31qmUSZuz7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Dataset')\n",
        "threshold = 0.5\n",
        "\n",
        "def binarize_predictions(predictions, threshold):\n",
        "    binary_predictions = []\n",
        "    for tensor in predictions:\n",
        "        binary_tensor = (tensor >= threshold).int()\n",
        "        binary_predictions.append(binary_tensor)\n",
        "    return binary_predictions\n",
        "model_checkpoint_path = 'model_checkpoint_2023-08-27_21-18-33.pth'\n",
        "model = DistilBert(768)\n",
        "# model.load_state_dict(torch.load(model_checkpoint_path, map_location=torch.device('cpu')))\n",
        "model.load_state_dict(torch.load(model_checkpoint_path))\n",
        "model.to(device)\n",
        "\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for (plots, genres_labels) in test_loader:\n",
        "        plots = pd.Series(plots)\n",
        "        tokenized = plots.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "        max_sequence_length = 512\n",
        "        tokenized = tokenized.apply(lambda seq: seq[:max_sequence_length])\n",
        "        max_len = 0\n",
        "        for i in tokenized.values:\n",
        "            if len(i) > max_len:\n",
        "                max_len = len(i)\n",
        "\n",
        "        padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "        attention_mask = np.where(padded != 0, 1, 0)\n",
        "        input_ids = torch.tensor(padded)\n",
        "        attention_mask = torch.tensor(attention_mask)\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        test_predictions.append(outputs)\n",
        "        test_labels.append(genres_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdZwzYudvC4g",
        "outputId": "d8a788ed-b7f3-4681-fc4f-23b84d58b3bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 Testing- compute F1-score"
      ],
      "metadata": {
        "id": "nsSmul_9cyF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = labels_debug\n",
        "test_predictions = predictions_debug\n",
        "print('一开始test_labels\\n\\n\\n',test_labels)\n",
        "threshold = 0.5\n",
        "def binarize_predictions(predictions, threshold):\n",
        "    binary_predictions = []\n",
        "    for tensor in predictions:\n",
        "        binary_tensor = (tensor >= threshold).int()\n",
        "        binary_predictions.append(binary_tensor)\n",
        "    return binary_predictions\n",
        "test_predictions = binarize_predictions(test_predictions, threshold)\n",
        "test_predictions = torch.cat(test_predictions, dim=0)\n",
        "test_labels = torch.cat(test_labels, dim=0)\n",
        "print('test_labels:\\n\\n', test_labels)\n",
        "print('test_label type:\\n\\n',type(test_labels))\n",
        "print('test_label shape:\\n\\n', np.array(test_labels).shape)\n",
        "print('test_predictions:\\n\\n', test_predictions)\n",
        "print('test_label type:\\n\\n',type(test_predictions))\n",
        "test_predictions = test_predictions.cpu()\n",
        "print('test_predictions shape:\\n\\n',np.array(test_predictions).shape)\n",
        "\n",
        "f1 = f1_score(test_labels, test_predictions, average=\"weighted\")\n",
        "print('weighted F1-score:', f1)\n",
        "microf1 = f1_score(test_labels, test_predictions, average=\"micro\")\n",
        "print('micro F1-score:', microf1)\n",
        "macrof1 = f1_score(test_labels, test_predictions, average=\"macro\")\n",
        "print('macro F1-score:', macrof1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZPL_xLL_DNd",
        "outputId": "4381e6ad-065a-4101-9690-37effff98158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "一开始test_labels\n",
            "\n",
            "\n",
            " [tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
            "        [1, 1, 0,  ..., 0, 0, 0],\n",
            "        [0, 1, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 1, 0,  ..., 0, 0, 0],\n",
            "        [0, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32), tensor([[0, 1, 1,  ..., 0, 0, 0],\n",
            "        [0, 1, 0,  ..., 0, 0, 0],\n",
            "        [1, 1, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 1,  ..., 0, 0, 0]], dtype=torch.int32), tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 1,  ..., 0, 0, 0],\n",
            "        [0, 1, 0,  ..., 0, 0, 0]], dtype=torch.int32), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 1, 0,  ..., 0, 0, 0],\n",
            "        [0, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 1, 0,  ..., 1, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32), tensor([[0, 1, 1,  ..., 0, 0, 0],\n",
            "        [0, 0, 1,  ..., 0, 0, 1],\n",
            "        [0, 1, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 1,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 1,  ..., 0, 0, 0]], dtype=torch.int32), tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32), tensor([[1, 1, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 1, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 1, 0,  ..., 0, 0, 0]], dtype=torch.int32), tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 1,  ..., 1, 0, 0]], dtype=torch.int32), tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 1, 0,  ..., 0, 0, 0]], dtype=torch.int32), tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 1,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 1, 0,  ..., 0, 0, 0],\n",
            "        [0, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 1, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 1,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32), tensor([[1, 0, 1,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 1, 0,  ..., 0, 0, 0],\n",
            "        [0, 1, 0,  ..., 0, 0, 0]], dtype=torch.int32), tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 1, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32), tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       dtype=torch.int32)]\n",
            "test_labels:\n",
            "\n",
            " tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
            "        [1, 1, 0,  ..., 0, 0, 0],\n",
            "        [0, 1, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 1, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)\n",
            "test_label type:\n",
            "\n",
            " <class 'torch.Tensor'>\n",
            "test_label shape:\n",
            "\n",
            " (993, 23)\n",
            "test_predictions:\n",
            "\n",
            " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 1,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 1, 0,  ..., 0, 0, 0],\n",
            "        [1, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32)\n",
            "test_label type:\n",
            "\n",
            " <class 'torch.Tensor'>\n",
            "test_predictions shape:\n",
            "\n",
            " (993, 23)\n",
            "weighted F1-score: 0.5444462339848982\n",
            "micro F1-score: 0.5658878504672897\n",
            "macro F1-score: 0.4361469601399069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the code to make 8k dataset, containing plot and genre\n"
      ],
      "metadata": {
        "id": "FsL31rIB1rok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import pickle\n",
        "from PIL import Image\n",
        "os.chdir('/content/drive/MyDrive/Dataset')\n",
        "file = h5py.File('multimodal_imdb.hdf5', 'r')\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import requests\n",
        "import random\n",
        "global classes\n",
        "classes = [\"Drama\", \"Comedy\", \"Romance\", \"Thriller\", \"Crime\", \"Action\", \"Adventure\", \"Horror\", \"Documentary\", \"Mystery\", \"Sci-Fi\", \"Music\", \"Fantasy\", \"Family\", \"Biography\", \"War\", \"History\", \"Animation\", \"Musical\", \"Western\", \"Sport\", \"Short\", \"Film-Noir\"]\n",
        "\n",
        "import json\n",
        "import shutil\n",
        "import pickle\n",
        "from PIL import Image\n",
        "\n",
        "# data2 folder\n",
        "dataset2_folder = 'mmimdb-rawdata/dataset/'\n",
        "\n",
        "# data1 element\n",
        "data_I = file['images']\n",
        "data_id = file['imdb_ids']\n",
        "data_g = file['genres']\n",
        "new_dataset = []\n",
        "# randomly select 8000 samples from over 20k dataset\n",
        "selected_indices = random.sample(range(len(data_I)), 8000)\n",
        "\n",
        "processed_data = []\n",
        "# This part involves integrating different forms of MM-IMDB dataset\n",
        "# because the .hdf5 file doesn't contain natural-language plot description, we have to download those from other source and combining them together\n",
        "for movie_index in selected_indices:\n",
        "\n",
        "    data_I_1 = np.array(data_I[movie_index])\n",
        "    data_I_show = np.transpose(data_I_1, (1, 2, 0))\n",
        "    image_float = data_I_show.astype(np.float32)\n",
        "    normalized_image = (image_float - np.min(image_float)) / (np.max(image_float) - np.min(image_float))\n",
        "    data_I_1 = (normalized_image * 255.0).astype(np.uint8)\n",
        "\n",
        "\n",
        "    movie_id = str(data_id[movie_index].decode('utf-8'))\n",
        "\n",
        "    data_g_1 = data_g[movie_index]\n",
        "\n",
        "    json_filename = f\"{movie_id}.json\"\n",
        "    json_filepath = os.path.join(dataset2_folder, json_filename)\n",
        "    director = None\n",
        "    plot = None\n",
        "    with open(json_filepath, 'r') as json_file:\n",
        "      json_data = json.load(json_file)\n",
        "      if 'director' in json_data:\n",
        "        director = json_data['director'][0]['name']\n",
        "      else:\n",
        "        continue\n",
        "      if 'plot' in json_data:\n",
        "        plot = json_data['plot'][0]\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "\n",
        "    # the structure of our rearraged 8kdataset, it is later saved in a pickle file.\n",
        "    processed_entry = {\n",
        "        'image': data_I_1,\n",
        "        'genre': data_g_1,\n",
        "        'director': director,\n",
        "        'plot': plot,\n",
        "        'imdb_id': movie_id,\n",
        "        'hdf5_index': movie_index\n",
        "    }\n",
        "\n",
        "    processed_data.append(processed_entry)\n",
        "\n",
        "\n",
        "pickle_filename = '8k_dataset.pkl'\n",
        "with open(pickle_filename, 'wb') as pickle_file:\n",
        "    pickle.dump(processed_data, pickle_file)\n",
        "\n",
        "print(\"it's been saved as:\", pickle_filename)\n",
        "\n"
      ],
      "metadata": {
        "id": "jUaxV7Zs1yJu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}